apiVersion: elastic.pytorch.org/v1alpha1
kind: ElasticJob
metadata:
  name: wandb-finbert-baseline
  #namespace: elastic-job
spec:
  # Use "etcd-service:2379" if you already apply etcd.yaml
  rdzvEndpoint: etcd-service:2379
  minReplicas: 1
  maxReplicas: 128
  replicaSpecs:
    Worker:
      replicas: 1
      restartPolicy: ExitCode
      template:
        apiVersion: v1
        kind: Pod
        spec:
          nodeSelector:
            beta.kubernetes.io/instance-type: p3.8xlarge
          containers:
          - name: elasticjob-worker
            image: 999701187340.dkr.ecr.us-west-2.amazonaws.com/torchelastic-huggingface
            imagePullPolicy: Always
            env:
            - name: NCCL_DEBUG
              value: INFO
              #  - name: NCCL_SOCKET_IFNAME
              #    value: lo
              #  - name: FI_PROVIDER
              #    value: sockets
            args:
            - "--nproc_per_node=2"
            - "/workspace/examples/huggingface/main.py"
            - "--epochs=1"
            - "--batch-size=16"
                # number of data loader workers (NOT trainers)
                # zero means load the data on the same process as the trainer
                # this is set so that the container does not OOM since
                # pytorch data loaders use shm
            - "--workers=6"
            - "/shared-efs/wandb-finbert/"
            resources:
              limits:
                nvidia.com/gpu: 2
            volumeMounts:
            - name: efs-pvc
              mountPath: /shared-efs
            - name: dshm
              mountPath: /dev/shm
          volumes:
          - name: efs-pvc
            persistentVolumeClaim:
              claimName: efs-claim
          - name: dshm
            emptyDir:
              medium: Memory
